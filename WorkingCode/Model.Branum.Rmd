---
title: "Model.Branum"
author: "Michael (Branum) Stephan"
date: "3/29/2020"
output: html_document
---

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(ROSE)
library(forcats)
library(MLeval)
library(gridExtra)
library(skimr)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
I'm going to spend the first part of this markdown making some reproducible functions for later analysis...

```{r}

# set model labels in order to easily identify them in other functions
set_model_label<- function(model, label){
  model$modelInfo$label <- label
}

# let's store this as a function as we progress through additional EDA
make_predictions <- function(model_list, test_dataframe, predCol){

# creating list output  
result = matrix(nrow = length(model_list), ncol=4)
x = 1

# iterate each item, predict, and gather statistics
for (item in model_list){
  # make model predictions
  predictions <- predict(item, newdata = test_dataframe)
  print(predictions)
  cm <- confusionMatrix(predictions, test_dataframe[,predCol])

  model_name <- item$modelInfo$label
  sensitivity <- cm$byClass["Sensitivity"]
  specificity <- cm$byClass["Specificity"]
  accuracy <- cm$overall["Accuracy"]
  
  result[x, 1] <- model_name
  result[x, 2] <- as.numeric(sensitivity)
  result[x, 3] <- as.numeric(specificity)
  result[x, 4] <- as.numeric(accuracy)
  x = x + 1
}
df_out <- as.data.frame(result)
colnames(df_out) = c("model_name", "sensitivity", "specificity", "accuracy")
return(df_out)
}

# create comparative bar plots for models based on their predictions
model_bar_plots <- function(df){
output_list = list()
i = 1
for(column in c("sensitivity", "specificity", "accuracy")){
p <- output %>% ggplot(aes_string(fill = "model_name", x = "model_name", y = column)) + geom_bar(stat = "identity") + ggtitle(paste("Model Comparison - ", column, sep = ""))
  output_list[[i]] = p
  i = i + 1
}
do.call(grid.arrange, output_list)
}

# build a function to generate ROC plots
generate_roc_plots <- function(model_list, names_list = NULL){
  
  # check for user default label entered
  if(is.null(names_list)){
    names_list = c()
    
    # iterate each model for their respective labels
    i = 1
    for(model in model_list){
    names_list[i] = model$modelInfo$label
    i = i + 1
    }
  }
  evalm(model_list, gnames=names_list)
}

# function to generate comparative bar plots and ROC plots
compare_models <- function(model_list, test_dataframe, predCol){
  # sensitivity, specificity, accuracy
  model_bar_plots(make_predictions(model_list, test_dataframe, predCol))
  
  # ROC
  generate_roc_plots(model_list)
}

```


```{r read in data files}
# create training set
df.train <- data.frame(read.csv("https://raw.githubusercontent.com/JaclynCoate/6372_Project_2/master/Data/Training_Test_Splits/banktrain_raw.csv"))
df.train <- select(df.train, -duration) #drop duration. This variable cannot be determined before the call is made so it cannot be included in models. 

# create test set
df.test <- data.frame(read.csv("https://raw.githubusercontent.com/JaclynCoate/6372_Project_2/master/Data/Training_Test_Splits/banktest_raw.csv"))
df.test <- select(df.test, -duration) #drop duration. This variable cannot be determined before the call is made so it cannot be included in models. 

# yes is not available in the training set
df.test <- df.test %>% filter(default != "yes")

# display training set
skimr::skim(df.train)
skimr::skim(df.test)
```
Training the base case logistic regression model
```{r}

# specifying internal cv
fitControl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = T,
  summaryFunction=twoClassSummary, 
  classProbs=T,
)


logistic.base <- train(Subscription ~., data=df.train, method="glm", family=binomial(), trControl=fitControl)
summary(logistic.base)
varImp(logistic.base)
```
Training the base case KNN model
```{r}
knn.base <- train(Subscription ~., data=df.train, method = "knn", trControl = fitControl, preProcess = c("center","scale"))

summary(knn.base)
```

Training the base case 
```{r}
rf.base <- train(Subscription ~., data=df.train, method="rf", trControl = fitControl)
summary(rf.base)
```
Now, it's time for our model comparison.

```{r}
set_model_label(knn.base, "Base KNN")
set_model_label(rf.base, "Base RF")
set_model_label(logistic.base, "Base MLR")

compare_models(list(knn.base, rf.base, logistic.base), df.test, "Subscription")
```

Based on the initial model, we have pretty good results, but the specificity is a little low. Let's see if we can add some features to enhance the prediction capabilities of the model and then recompare. First, let's remove non-significant variables based on the statistical significance of the logistic regression.
```{r}
summary(logistic.base)
```
It appears that job status, default status, month, day of week, emp_var_rate, cons_price_idx, and euribor3m are all significant, so let's start by using only these values and retraining some new models and comparing.
```{r}
# creating a reduced set size
df.train.reduced <- df.train %>% select(Subscription, job, default, month, day_of_week, emp_var_rate, cons_price_idx, euribor3m, poutcome)

df.test.reduced <- df.test %>% select(Subscription, job, default, month, day_of_week, emp_var_rate, cons_price_idx, euribor3m, poutcome)
```

Now, let's train our models
```{r}

# training with a reduced data set
logistic.reduced <- train(Subscription ~., data=df.train.reduced, method="glm", family=binomial(), trControl=fitControl)
knn.reduced <- train(Subscription ~., data=df.train.reduced, method = "knn", trControl = fitControl, preProcess = c("center","scale"))
rf.reduced <- train(Subscription ~., data=df.train.reduced, method="rf", trControl = fitControl)

```
Let's compare our reduced models and see how we did.
```{r}
compare_models(list(knn.reduced, rf.reduced, logistic.reduced), df.test.reduced, "Subscription")

```
In looking at our reduced model, we don't necessarily see any real benefit in performance other than a reduction in model complexity. Let's next try the models with the "final" from the eda file and see how that compares to the reduced "base case" scenario.

```{r}

# add interaction for job/age and month/poutcome
df.train.engineered <- df.train.reduced %>% 
  mutate(optimal_months = case_when(month == "sep" ~ 1, month == "oct" ~ 1, month == "mar" ~ 1, month == "dec" ~ 1, TRUE ~ 0), month_and_campaign = paste(optimal_months, poutcome, sep="_"), senior = case_when(df.train$age > 60 ~ 1, TRUE ~0), job_seniority = paste(job, senior, sep="_"),
defaulted = case_when(default == "unknown" ~ 1, default == "no" ~ 1, TRUE ~ 0)) %>% 
  select(-one_of(c("poutcome", "month", "optimal_months", "job", "senior", "day_of_week", "default", "cons_price_idx", "euribor3m")))

df.test.engineered <- df.test.reduced %>% 
  mutate(optimal_months = case_when(month == "sep" ~ 1, month == "oct" ~ 1, month == "mar" ~ 1, month == "dec" ~ 1, TRUE ~ 0), month_and_campaign = paste(optimal_months, poutcome, sep="_"), senior = case_when(df.test$age > 60 ~ 1, TRUE ~0), job_seniority = paste(job, senior, sep="_"),
defaulted = case_when(default == "unknown" ~ 1, default == "no" ~ 1, TRUE ~ 0)) %>% 
  select(-one_of(c("poutcome", "month", "optimal_months", "job", "senior", "day_of_week", "default", "cons_price_idx", "euribor3m")))
```
Let's check out our new data sets

```{r}
skim(df.train.engineered)
```

```{r}
# training with a reduced data set
logistic.engineered <- train(Subscription ~., data=df.train.engineered, method="glm", family=binomial(), trControl=fitControl)
knn.engineered <- train(Subscription ~., data=df.train.engineered, method = "knn", trControl = fitControl, preProcess = c("center","scale"))
rf.engineered <- train(Subscription ~., data=df.train.engineered, method="rf", trControl = fitControl)
```

```{r}
compare_models(list(knn.engineered, rf.engineered, logistic.engineered), df.test.engineered, "Subscription")
```

```{r}
# TODO: ADDRESS MISSING LEVELS IN TRAIN SET
predict(rf.engineered, newdata=df.test.engineered)
```

```{r}
fitControl <- trainControl(## 10-fold CV
  method = "cv",
  number = 10,
  savePredictions = TRUE
)

model.noNumeric.balanced.2 <- train(Subscription ~., data=df.noNumeric.balanced.2.train %>% 
                                      select(Subscription, student_retired, default.grouped, illiterate, prime_months,                                         succesful_campaign,age.senior, euribor3m.grouped), method="glm", family=binomial(),                                       trControl=fitControl)
```

```{r}
summary(model.noNumeric.balanced.2)
varImp(model.noNumeric.balanced.2)
```

```{r}
predictions.noNumeric.balanced.2 <- predict(model.noNumeric.balanced.2, df.noNumeric.balanced.2.test)
confusionMatrix(predictions.noNumeric.balanced.2, df.noNumeric.balanced.2.test$Subscription)
```



### (JTE) ROC Curve Examples

3 Different pacakges for generating ROC Curves. I tried for a while to get any to work going straight from the models run above, but I cant get any of the generated predictions to work with these. 

```{r}
#Have to simplify model code to generate correctly foramtted predictions for ROC packages. For some reason predictions from CV methods above do not work.
model<-glm(Subscription ~.,  data=df.noNumeric.balanced.2.train,family = "binomial")
```

```{r ROC Curves using ROCit}
#So far I like this one best
library(ROCit)

ROCit_obj <- rocit(score=predict(model),class=df.noNumeric.balanced.2.train$Subscription)
plot(ROCit_obj)
text(x = .40, y = .6,paste("AUC = ", round(ROCit_obj$AUC,3), sep = ""))
```

```{r ROC Curves using ROCR}
#This is the one used in Turner's "AutoClassify.R" file in Unit 12.
library(ROCR)

pred <- prediction(predict(model), df.noNumeric.balanced.2.train$Subscription)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
plot(roc.perf,main="ROC Plot")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#not sure why the last line doesnt work - for this run AUC is .792
```

```{r ROC Curves using precrec}
library(precrec)

precrec_obj <- evalmod(scores = predict(model), labels = df.noNumeric.balanced.2.train$Subscription)
autoplot(precrec_obj)
```



There appears to be a pretty significant increase in specificity and now our model appears to be much more balanced. The next steps are some additional exploration on the new features to experiment with cutoffs for each group.

```{r Categorical Variable Review Grid}
df.noNumeric.balanced.2 %>% ggplot(aes(x=education, fill = Subscription)) + geom_bar(position = "fill", alpha = 0.9) + coord_flip()

df.noNumeric.balanced.2 %>% ggplot(aes(x=illiterate, fill = Subscription)) + geom_bar(position = "fill", alpha = 0.9) + coord_flip()
```
Looking at our illiterate group, there is some slight seperation, but nothing super signficant.
```{r 'Categorical Variable Review Grid', fig.width = 6,fig.height = 10}
df.noNumeric.balanced.2 %>% 
  mutate(job_education = paste(df.noNumeric.balanced.2$job, df.noNumeric.balanced.2$education, sep="_")) %>% 
  ggplot(aes(x=fct_reorder(job_education, as.numeric(Subscription), fun = mean))) + geom_bar(position = "fill", alpha = 0.9) + coord_flip() + ggtitle("Profession and Education vs. Subscription")
```
Let's see if there is any interaction that might be present in the data.







